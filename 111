import asyncio
import os
import threading
import time
import traceback
import uuid
from collections import deque
from concurrent.futures import Future
from dataclasses import dataclass
from typing import AsyncGenerator, Generic, List, Literal, Optional, Tuple, TypeVar
from weakref import WeakValueDictionary

import torch
from tqdm import tqdm

from GPT_SoVITS.AR.models.t2s_model_abc import KVCacheNHD as KVCache
from GPT_SoVITS.AR.models.t2s_model_abc import Sampler, T2SDecoderABC, TorchProfiler

Tensor = torch.Tensor
K = TypeVar("K")
V = TypeVar("V")

try:
    import uvloop
except ImportError:
    pass
else:
    asyncio.set_event_loop(uvloop.new_event_loop())


class DQCache(Generic[K, V]):
    def __init__(self, max_size: int = 16):
        self.max_size = max_size
        self.cache: WeakValueDictionary[K, V] = WeakValueDictionary()
        self.dq: deque[Tuple[K, V]] = deque(maxlen=max_size)

    def __setitem__(self, request_id: K, session: V):
        self.cache[request_id] = session
        if len(self.dq) == 16:
            evicted = self.dq.popleft()[0]
            self.cache.pop(evicted)
            self.dq.append((request_id, session))

    def __getitem__(self, request_id: K) -> Optional[V]:
        return self.cache[request_id]


@dataclass
class T2SResult:
    result: List[Tensor]
    status: Literal["Success", "Error", "Cancelled"] = "Success"
    exception: Optional[Exception] = None
    traceback: Optional[str] = None


@dataclass
class T2SRequest:
    x: List[torch.Tensor]
    x_lens: Tensor
    prompts: torch.Tensor
    bert_feature: List[Tensor]
    valid_length: int
    top_k: int = 5
    top_p: int = 1
    early_stop_num: int = -1
    temperature: float = 1.0
    repetition_penalty: float = 1.35
    use_cuda_graph: bool = False
    debug: bool = False
    request_id = str(uuid.uuid1())


class T2SSession:
    def __init__(self, decoder: T2SDecoderABC, request: T2SRequest):
        self.decoder = decoder
        self.request = request

        bsz = len(self.x)
        y_len = self.y.size(-1)
        self.bsz = bsz
        self.y_len = y_len

        # Cache
        self.kv_cache = decoder.init_cache(KVCache, bsz)  # type: ignore
        self.sampler = Sampler(bsz, decoder.vocab_size)

        # Forward args
        self.x = request.x
        self.x_lens = request.x_lens.to(torch.int64)
        self.y = request.prompts
        self.bert_feature = request.bert_feature

        self.prefill_len = self.x_lens + self.y.size(1)

        self.input_pos = decoder.h.input_pos.clone()
        self.input_pos.add_(self.prefill_len)

        # CUDA Graph
        self.graph: Optional[torch.cuda.CUDAGraph] = None
        self.xy_pos_ = decoder.h.xy_pos.clone()
        self.xy_dec_ = decoder.h.xy_dec.clone()

        # EOS
        self.completed = [False] * len(self.x)
        self.y_results: List[Tensor] = [None] * len(self.x)  # type: ignore

        self.xy_pos = decoder.embed(self.x, self.y, self.bert_feature)

        attn_mask = []
        for bs in range(bsz):
            pos = int(self.x_lens[bs].item())
            mask = torch.zeros(pos + y_len, pos + y_len, device=self.xy_pos.device).bool()
            mask[:, :pos].fill_(True)
            mask[-y_len:, -y_len:] = ~torch.triu(
                torch.ones(y_len, y_len, device=self.xy_pos.device, dtype=torch.bool), diagonal=1
            )
            attn_mask.append(mask)
        self.attn_mask_nested = torch.nested.nested_tensor(attn_mask)


class AsyncT2SEngine:
    def __init__(self, decoder_model: T2SDecoderABC):
        self.decoder_model = decoder_model
        self.background_loop = asyncio.new_event_loop()
        self.sessions: DQCache[str, T2SSession] = DQCache()
        self.futures: WeakValueDictionary[str, Future] = WeakValueDictionary()
        self.lock = asyncio.Lock()
        self.semaphore = asyncio.Semaphore(20)

        self.loop_thread = threading.Thread(target=self._run_event_loop, daemon=True)
        self.loop_thread.start()

    def _run_event_loop(self):
        asyncio.set_event_loop(self.background_loop)
        self.background_loop.run_forever()

    async def _handle_request(self, request: T2SRequest):
        async with self.lock:
            decoder = self.decoder_model
            session = T2SSession(decoder, request)
            self.sessions[request.request_id] = session

        y = session.y
        bsz = y.size(0)
        t1 = 0.0

        torch_profiler = TorchProfiler(request.debug)

        with torch_profiler.profiler():
            for idx in tqdm(range(1500)):
                if idx == 0:
                    xy_dec = decoder.h.prefill(session.xy_pos, session.attn_mask_nested, session.kv_cache)
                    xy_dec = torch.stack([t[[-1]] for t in xy_dec.unbind()])
                else:
                    if request.use_cuda_graph and session.graph is None and torch.cuda.is_available():
                        session.xy_pos_.copy_(session.xy_pos)
                        session.graph = decoder.capture(
                            session.input_pos, session.xy_pos_, session.xy_dec_, kv_caches=session.kv_cache
                        )

                    torch_profiler.start()
                    with torch_profiler.record("AR"):
                        if session.graph:
                            session.xy_pos_.copy_(session.xy_pos)
                            session.graph.replay()
                            xy_dec = session.xy_dec_.clone()
                        else:
                            xy_dec = decoder.h.forward(session.input_pos, session.xy_pos, session.kv_cache)

                logits = decoder.ar_predict_layer(xy_dec[:, -1])
                session.input_pos.add_(1)

                if idx == 0:
                    logits = logits[:, :-1]

                with torch_profiler.record("Sampling"):
                    samples = session.sampler.sample(
                        logits=logits,
                        previous_tokens=session.y,
                        top_k=request.top_k,
                        top_p=request.top_p,
                        repetition_penalty=request.repetition_penalty,
                        temperature=request.temperature,
                        use_cuda_graph=session.graph is not None,
                        idx=idx,
                    )

                    session.y = torch.cat([session.y, samples], dim=1)  # type: ignore

                with torch_profiler.record("EOS"):
                    EOS_mask = (samples[:, 0] == decoder.EOS) | (torch.argmax(logits, dim=-1) == decoder.EOS)
                    EOS_indices: List[int] = torch.where(EOS_mask)[0].tolist()

                    for i in EOS_indices:
                        if not session.completed[i]:
                            session.y_results[i] = session.y[i, session.y_len : -1]
                            session.completed[i] = True

                    if all(session.completed):
                        if session.y.size(1) == 0:
                            session.y = torch.cat([session.y, torch.zeros_like(samples)], dim=1)
                            tqdm.write("Bad Zero Prediction")
                        else:
                            tqdm.write(
                                f"T2S Decoding EOS {session.prefill_len.tolist().__str__().strip('[]')} -> \n{[i.size(0) for i in session.y_results].__str__().strip('[]')}"
                            )
                            tqdm.write(f"Infer Speed: {(idx - 1) / (time.perf_counter() - t1):.2f}")
                        break

                    if request.early_stop_num != -1 and (session.y.size(1) - session.y_len) > request.early_stop_num:
                        for i in range(bsz):
                            if not session.completed[i]:
                                session.y_results[i] = session.y[i, session.y_len :]
                                session.completed[i] = True
                        break

                with torch_profiler.record("NextPos"):
                    y_emb = decoder.ar_audio_embedding(session.y[:, -1:])
                    session.xy_pos = decoder.ar_audio_position.forward(session.input_pos - session.x_lens, y_emb)

                if idx == 2:
                    t1 = time.perf_counter()

                if idx == 51:
                    torch_profiler.end()

                if idx % 10 == 0:
                    await asyncio.sleep(0)

            async with self.lock:
                self.futures.pop(request.request_id, None)

            return session.y_results

    async def generate(self, request: T2SRequest) -> AsyncGenerator[T2SResult]:
        try:
            async with self.semaphore:
                future = asyncio.run_coroutine_threadsafe(self._handle_request(request), self.background_loop)
                async with self.lock:
                    self.futures[request.request_id] = future
                result = future.result()
                t2s_result = T2SResult(result, "Success")
        except asyncio.CancelledError:
            t2s_result = T2SResult([], "Cancelled")
        except Exception as e:
            t2s_result = T2SResult([], "Error", exception=e, traceback=traceback.format_exc())
        yield t2s_result

    async def cancel(self, request_id: str):
        async with self.lock:
            future = self.futures.pop(request_id, None)
        if future and not future.done():
            future.cancel()

    @classmethod
    def from_pretrained(cls, weights_path: os.PathLike, implement: str):
        return cls(cls.load_decoder(weights_path, implement))

    @staticmethod
    def load_decoder(weights_path: os.PathLike, implement: str):
        print(f"Loading Text2Semantic Weights from {weights_path} with {implement.replace('_', ' ').title()} Implement")
        module_path = f"GPT_SoVITS.AR.models.t2s_model_{implement.lower()}"
        cls_name = "T2SDecoder"
        mod = __import__(module_path, fromlist=[cls_name])
        decoder_cls: T2SDecoderABC = getattr(mod, cls_name)
        dict_s1 = torch.load(weights_path, map_location="cpu", weights_only=False, mmap=True)
        config = dict_s1["config"]
        decoder: T2SDecoderABC = decoder_cls(config, max_batch_size=20)
        state_dict = dict_s1["weight"]
        decoder.load_state_dict(state_dict)
        return decoder.eval()


import asyncio
import threading

from GPT_SoVITS.BigVGAN.bigvgan import BigVGAN
from GPT_SoVITS.module.models import SynthesizerTrn, SynthesizerTrnV3

try:
    import uvloop
except ImportError:
    pass
else:
    asyncio.set_event_loop(uvloop.new_event_loop())


class AsyncSoVITSEngine:
    def __init__(self, sovits_model: SynthesizerTrn | SynthesizerTrnV3) -> None:
        self.sovits_model = sovits_model

        self.background_loop = asyncio.new_event_loop()
        self.lock = asyncio.Lock()
        self.semaphore = asyncio.Semaphore(20)

        self.loop_thread = threading.Thread(target=self._run_event_loop, daemon=True)
        self.loop_thread.start()

    def _run_event_loop(self):
        asyncio.set_event_loop(self.background_loop)
        self.background_loop.run_forever()

    def load_bigvgan(self, bigvgan_path: str):
        if self.bigvgan_model is not None:
            return
        self.bigvgan_model = BigVGAN.from_pretrained(bigvgan_path, use_cuda_kernel=False)
        self.bigvgan_model.remove_weight_norm()
        self.bigvgan_model = self.bigvgan_model.eval()


import asyncio
import os
import pickle
import threading
from typing import Type

import torch
import torch.nn as nn
from peft.mapping import get_peft_model
from peft.tuners.lora.config import LoraConfig
from transformers import (
    AutoModelForMaskedLM,
    AutoTokenizer,
    BertForMaskedLM,
    PreTrainedTokenizer,
    PreTrainedTokenizerFast,
)

from GPT_SoVITS.AR.models.t2s_model_async import AsyncT2SEngine
from GPT_SoVITS.BigVGAN.bigvgan import BigVGAN
from GPT_SoVITS.feature_extractor.cnhubert import CNHubert
from GPT_SoVITS.module.models import SynthesizerTrn, SynthesizerTrnV3
from tools.cfg import PRETRAINED_SOVITS_V3, API_Cfg, Inference_WebUI_Cfg, Speakers_Cfg
from tools.my_utils import DictToAttrRecursive

try:
    import uvloop
except ImportError:
    pass
else:
    asyncio.set_event_loop(uvloop.new_event_loop())


class SafePKUnpickler(pickle.Unpickler):
    def __init__(self, file):
        self._file = file
        self._prefix = self._file.read(2)

        if self._prefix == b"PK":
            self._file.seek(0)
            self._file[0:2] = b"PK"
        else:
            pass

        super().__init__(self._file)


class AsyncTTSEngine:
    def __init__(self, cfg: API_Cfg | Inference_WebUI_Cfg, speaker_cfg: Speakers_Cfg, implement="flash_attn") -> None:
        speaker_name = cfg.speaker_name
        self.speaker = speaker_cfg.get_speaker(speaker_name)

        self.t2s_engine = AsyncT2SEngine.from_pretrained(self.speaker.t2s_path, implement)
        self.vits_model: SynthesizerTrn | SynthesizerTrnV3
        self.vocoder: BigVGAN

        self.background_loop = asyncio.new_event_loop()
        self.lock = asyncio.Lock()
        self.semaphore = asyncio.Semaphore(20)

        self.loop_thread = threading.Thread(target=self._run_event_loop, daemon=True)
        self.loop_thread.start()

    def _run_event_loop(self):
        asyncio.set_event_loop(self.background_loop)
        self.background_loop.run_forever()

    async def load_t2s_model(self, weights_path: os.PathLike, implement: str = "flash_attn"):
        async with self.lock:
            self.t2s_engine.decoder_model = AsyncT2SEngine.load_decoder(weights_path, implement)

    def load_hubert(self, weights_path: os.PathLike):
        print(f"Loading CNHuBERT weights from {weights_path}")
        self.cnhuhbert_model = CNHubert(weights_path)
        self.cnhuhbert_model = self.cnhuhbert_model.eval()

    def load_bert(self, weights_path: os.PathLike):
        print(f"Loading BERT weights from {weights_path}")
        self.bert_tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(weights_path)
        self.bert_model: BertForMaskedLM = AutoModelForMaskedLM.from_pretrained(weights_path)
        self.bert_model = self.bert_model.eval()

    def load_vocoder(self, vocoder_cls: Type[BigVGAN]):
        if isinstance(self.vocoder, vocoder_cls):
            return

        match vocoder_cls:
            case BigVGAN:
                self.vocoder = BigVGAN.from_pretrained(self.speaker.bigvgan, use_cuda_kernel=False)
                # remove weight norm in the model and set to eval mode
                self.vocoder.remove_weight_norm()
                self.vocoder = self.vocoder.eval()

    async def load_sovits_model(self, weights_path: os.PathLike):
        async with self.lock:
            print(f"Loading SoVITS weights from {weights_path}")
            dict_s2 = torch.load(
                weights_path, map_location="cpu", weights_only=False, mmap=True, pickle_module=SafePKUnpickler
            )
            hps = dict_s2["config"]
            hps = DictToAttrRecursive(hps)
            hps.model.semantic_frame_rate = "25hz"
            self.hps = hps
            kwds = hps.model

            version = "v0"
            is_lora = False

            if "dec.conv_pre.weight" in dict_s2["weight"].keys():
                if dict_s2["weight"]["enc_p.text_embedding.weight"].shape[0] == 322:
                    version = "v1"
                else:
                    version = "v2"
            elif "cfm.estimator.proj_out.bias" in dict_s2["weight"].keys():
                if "cfm.estimator.proj_out.bias" in dict_s2["weight"].keys():
                    version = "v3"

            hps.model.version = version

            if "lora_rank" in dict_s2.keys():
                is_lora = True

            match version:
                case "v1", "v2":
                    self.vits_model = SynthesizerTrn(
                        hps.data.filter_length // 2 + 1,
                        hps.train.segment_size // hps.data.hop_length,
                        n_speakers=hps.data.n_speakers,
                        **kwds,
                    )
                case "v3":
                    self.vits_model = SynthesizerTrnV3(
                        hps.data.filter_length // 2 + 1,
                        hps.train.segment_size // hps.data.hop_length,
                        n_speakers=hps.data.n_speakers,
                        **kwds,
                    )
                    self.load_bigvgan()
                    if "pretrained" not in str(weights_path) and hasattr(self.vits_model, "enc_q"):
                        del self.vits_model.enc_q

            if is_lora:
                print(f"Loading VITS pretrained weights from {weights_path}")
                print(
                    f"{self.vits_model.load_state_dict(torch.load(PRETRAINED_SOVITS_V3, map_location='cpu', pickle_module=SafePKUnpickler)['weight'], strict=False)}"
                )
                lora_rank = dict_s2["lora_rank"]
                lora_config = LoraConfig(
                    target_modules=["to_k", "to_q", "to_v", "to_out.0"],
                    r=lora_rank,
                    lora_alpha=lora_rank,
                    init_lora_weights=False,
                )
                self.vits_model.cfm = get_peft_model(self.vits_model.cfm, lora_config, low_cpu_mem_usage=True)  # type: ignore
                print(f"Loading LoRA weights from {weights_path}")
                print(f"{self.vits_model.load_state_dict(dict_s2['weight'], strict=False)}")
                self.vits_model.cfm = self.vits_model.cfm.merge_and_unload()  # type: ignore
            else:
                print(f"Loading VITS weights from {weights_path}")
                print(self.vits_model.load_state_dict(dict_s2["weight"], strict=False))

            self.vits_model.eval()


from __future__ import annotations

import os
import warnings
from functools import partial
from pathlib import Path
from typing import Annotated, Dict, Hashable, Literal, Optional, TypeVar

import torch
from pydantic import AfterValidator, BaseModel, Field, model_validator
from pydantic_core import PydanticCustomError

from tools.i18n.i18n import scan_language_list
from tools.my_utils import check_infer_device

PRETRAINED_BASE = Path("GPT_SoVITS/pretrained_models")

CNHUBERT_DEFAULT = PRETRAINED_BASE / "chinese-hubert-base"
BERT_DEFAULT = PRETRAINED_BASE / "chinese-roberta-wwm-ext-large"
BIGVGAN_DEFAULT = PRETRAINED_BASE / "models--nvidia--bigvgan_v2_24khz_100band_256x"

PRETRAINED_T2S_V1 = PRETRAINED_BASE / "s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"
PRETRAINED_T2S_V2 = PRETRAINED_BASE / "gsv-v2final-pretrained/s1bert25hz-5kh-longer-epoch=12-step=369668.ckpt"
PRETRAINED_T2S_V3 = PRETRAINED_BASE / "s1v3.ckpt"
PRETRAINED_SOVITS_V1 = PRETRAINED_BASE / "s2G488k.pth"
PRETRAINED_SOVITS_V2 = PRETRAINED_BASE / "gsv-v2final-pretrained/s2G2333k.pth"
PRETRAINED_SOVITS_V3 = PRETRAINED_BASE / "s2Gv3.pth"

V1_LANGUAGES = [
    "all_zh",  # All Chinese
    "en",  # All English
    "all_ja",  # All Japanese
    "zh",  # Mixed Chinese & English
    "ja",  # Mixed Japanese & English
    "auto",  # Auto Detect Language
]
V2_LANGUAGES = [
    "all_zh",  # All Chinese
    "en",  # All English
    "all_ja",  # All Japanese
    "all_yue",  # All Cantonese
    "all_ko",  # All Korean
    "zh",  # Mixed Chinese & English
    "ja",  # Mixed Japanese & English
    "yue",  # Mixed Cantonese & English
    "ko",  # Mixed Korean & English
    "auto",  # Auto Detect Language (Without Cantonese)
    "auto_yue",  # Auto Detect Language (with Cantonese)
]

T = TypeVar("T", bound=Hashable)


def _validate_unique_list(v: list[T]) -> list[T]:
    if len(v) != len(set(v)):
        raise PydanticCustomError("unique_list", "list must be unique")
    return v


Uniquelist = Annotated[list[T], AfterValidator(_validate_unique_list)]


class Main_WebUI_Cfg(BaseModel):
    init: bool = Field(default=False, exclude=True)
    cuda: bool = Field(default=False)
    cuda_visible_device: list[int] = Field(default_factory=list, exclude=not torch.cuda.is_available())
    fp16: bool = False
    logging_dir: str = "logs"
    python_exec: str = "python3"
    host: str = "0.0.0.0"
    webui_port: Annotated[int, Field(ge=0, le=65536, strict=True)] = 9874
    uvr5_webui_port: Annotated[int, Field(ge=0, le=65536, strict=True)] = 9873
    subfux_webui_port: Annotated[int, Field(ge=0, le=65536, strict=True)] = 9871
    i18n_language: str = "en_US"
    gradio_share: bool = False

    @model_validator(mode="after")
    @classmethod
    def check_all(cls, vals):
        if vals.init:
            _, vals.fp16 = check_infer_device()
            if torch.cuda.is_available():
                vals.cuda = True
                vals.cuda_visible_device = list(range(0, torch.cuda.device_count()))
        vals.init = False

        if not torch.cuda.is_available():
            vals.cuda = False
        if not vals.cuda:
            vals.cuda_visible_device = []
            vals.fp16 = False

        assert vals.i18n_language in scan_language_list(), ValueError("I18n language not supported")
        return vals


class Inference_WebUI_Cfg(BaseModel):
    init: bool = Field(default=False, exclude=True)
    device: str = "cpu"
    fp16: bool = False
    host: str = "0.0.0.0"
    port: Annotated[int, Field(ge=0, le=65536, strict=True)] = 9872
    i18n_language: str = "en_US"
    gradio_share: bool = False
    speaker_name: str = "WebUI"

    # Exclude
    streaming: bool = Field(default=False, exclude=True)
    top_k: Annotated[int, Field(ge=1, le=100, strict=True)] = Field(default=5, exclude=True)
    top_p: Annotated[float, Field(ge=0.01, le=1.0, strict=True)] = Field(default=1.0, exclude=True)
    temperature: Annotated[float, Field(ge=0.01, le=1.0, strict=True)] = Field(default=1.0, exclude=True)
    text_splitting_method: Annotated[str, Field(pattern=r"^cut[0-5]$")] = Field(default="cut1", exclude=True)
    batch_size: int = Field(default=4, exclude=True)
    speed_factor: Annotated[float, Field(ge=0.6, le=1.4, strict=True)] = Field(default=1.0, exclude=True)
    fragment_interval: Annotated[float, Field(ge=0.01)] = Field(default=0.3, exclude=True)
    parallel_inference: bool = Field(default=True, exclude=True)
    reprtition_penalty: Annotated[float, Field(ge=0.9, le=2.0, strict=True)] = Field(default=1.35, exclude=True)
    sample_steps: Literal[4, 8, 16, 32] = Field(default=32, exclude=True)
    super_samplings: bool = Field(default=False, exclude=True)
    media_type: Literal["pcm", "wav", "aac", "ogg"] = Field(default="pcm", exclude=True)

    @model_validator(mode="after")
    @classmethod
    def check_all(cls, vals):
        if vals.init:
            vals.device, vals.fp16 = check_infer_device()
        vals.init = False
        device = vals.device
        if device == "cpu":
            pass
        elif device.startswith("cuda"):
            pass
        elif device.startswith("mps"):
            pass
        else:
            raise ValueError(f"Invalid device: {device}")
        assert 0 < vals.port < 65536, ValueError("Invalid Port")
        assert vals.i18n_language in scan_language_list(), ValueError("I18n language not supported")

        return vals


class API_Cfg(BaseModel):
    init: bool = Field(default=False, exclude=True)
    device: str = "cpu"
    fp16: bool = False
    host: str = "::"
    port: Annotated[int, Field(ge=0, le=65536, strict=True)] = 9880
    i18n_language: str = "en_US"
    speaker_name: str = "API"
    streaming: bool = False
    media_type: Literal["pcm", "wav", "aac", "ogg"] = "wav"
    top_k: Annotated[int, Field(ge=1, le=100, strict=True)] = 5
    top_p: Annotated[float, Field(ge=0.01, le=1.0, strict=True)] = 1.0
    temperature: Annotated[float, Field(ge=0.01, le=1.0, strict=True)] = 1.0
    text_splitting_method: Annotated[str, Field(pattern=r"^cut[0-5]$")] = "cut1"
    batch_size: Annotated[int, Field(ge=1, strict=True)] = 5
    speed_factor: Annotated[float, Field(ge=0.6, le=1.4, strict=True)] = 1.0
    fragment_interval: Annotated[float, Field(ge=0.01, le=4.0)] = 0.3
    parallel_inference: bool = True
    reprtition_penalty: Annotated[float, Field(ge=0.9, le=2.0, strict=True)] = 1.35
    sample_steps: Literal[4, 8, 16, 32] = 32
    super_samplings: bool = False

    @model_validator(mode="after")
    @classmethod
    def check_all(cls, vals):
        if vals.init:
            vals.device, vals.fp16 = check_infer_device()
        vals.init = False
        device = vals.device
        if device == "cpu":
            pass
        elif device.startswith("cuda"):
            pass
        elif device.startswith("mps"):
            pass
        else:
            raise ValueError(f"Invalid device: {device}")
        assert vals.i18n_language in scan_language_list(), ValueError("I18n language not supported")
        return vals


class Prompt(BaseModel):
    ref_audio_path: Optional[str] = Field(
        default=None, examples=[None], description="Reference Audio Path for the Speaker"
    )
    prompt_text: Optional[str] = Field(default=None, examples=[None], description="Text for the Ref Audio, Optional")
    prompt_lang: Optional[str] = Field(
        default=None, examples=[None], description="Language for the Ref Audio, Required iff Prompt Text is not None"
    )
    aux_ref_audio_paths: Annotated[Uniquelist[str], Field(min_length=0, max_length=10)] = Field(
        default_factory=list, examples=[[]]
    )

    # Exclude
    version: Literal["v1", "v2", "v3"] = Field("v3", exclude=True)

    def is_empty(self) -> bool:
        return not self.ref_audio_path

    @model_validator(mode="before")
    @classmethod
    def check_all(cls, vals):
        if vals.ref_audio_path and not os.path.exists(vals.ref_audio_path):
            vals.ref_audio_path = None
            vals.prompt_text = None
            vals.prompt_lang = None
            vals.aux_ref_audio_paths = []
            warnings.warn("Ref audio not found", UserWarning)
        if vals.ref_audio_path:
            if vals.prompt_text and vals.prompt_lang:
                pass
            else:
                vals.prompt_text = None
                vals.prompt_lang = None
        return vals


Prompt_p = partial(Prompt, version="v3")


class Speaker(BaseModel):
    t2s_path: Path = PRETRAINED_T2S_V3
    sovits_path: Path = PRETRAINED_SOVITS_V3
    version: Literal["v1", "v2", "v3"] = "v3"
    is_lora: bool = True
    prompt: Prompt = Field(default_factory=Prompt_p)

    cnhubert: Path = Field(default=CNHUBERT_DEFAULT, exclude=True)
    bert: Path = Field(default=BERT_DEFAULT, exclude=True)
    bigvgan: Path = Field(default=BIGVGAN_DEFAULT, exclude=True)

    languages: list = Field(default_factory=list, exclude=True)

    @model_validator(mode="before")
    @classmethod
    def check_lang(cls, vals):
        version = vals.version
        match version:
            case "v1":
                vals.languages = V1_LANGUAGES
            case "v2":
                vals.languages = V2_LANGUAGES
            case "v3":
                vals.languages = V2_LANGUAGES

        prompt_lang = vals.prompt.prompt_lang
        if prompt_lang is not None:
            assert prompt_lang in vals.languages, ValueError("Invalid prompt language")

        vals.prompt.verison = version
        if version == "v3" and vals.prompt.aux_ref_audio_paths:
            vals.prompt.aux_ref_audio_paths = []
            warnings.warn("Auxiliary Ref Audio Not Supported in V3", UserWarning)

        if not os.path.exists(vals.t2s_path):
            warnings.warn("File not found, falling back to PRETRAINED_T2S.", UserWarning)
            match version:
                case "v1":
                    vals.t2s_path = PRETRAINED_T2S_V1
                case "v2":
                    vals.t2s_path = PRETRAINED_T2S_V2
                case "v3":
                    vals.t2s_path = PRETRAINED_T2S_V3

        if not os.path.exists(vals.sovits_path):
            warnings.warn("File not found, falling back to PRETRAINED_SoVITS.", UserWarning)
            match version:
                case "v1":
                    vals.sovits_path = PRETRAINED_SOVITS_V1
                case "v2":
                    vals.sovits_path = PRETRAINED_SOVITS_V2
                case "v3":
                    vals.sovits_path = PRETRAINED_SOVITS_V3

        return vals

    def update_language(self):
        version = self.version
        match version:
            case "v1":
                self.languages = V1_LANGUAGES
            case "v2":
                self.languages = V2_LANGUAGES
            case "v3":
                self.languages = V2_LANGUAGES


class Speakers_Cfg(BaseModel):
    speakers_dict: Dict[str, Speaker] = {
        "WebUI": Speaker(),
        "API": Speaker(),
    }
    file_path: str = Field(default="./tools/cfgs/speakers.json", exclude=True)

    def get_speaker(self, speaker_name: str):
        return self.speakers_dict.get(speaker_name, Speaker())

    def list_speaker(self):
        return self.speakers_dict.keys()

    @model_validator(mode="before")
    @classmethod
    def check_all(cls, vals):
        default_dict = {
            "WebUI": Speaker(),
            "API": Speaker(),
        }
        vals.speakers_dict = {**default_dict, **vals.speakers_dict}

        return vals

    @classmethod
    def from_json(cls, file_path: str) -> "Speakers_Cfg":
        if not os.path.exists(file_path):
            cls().save_as_json(file_path=file_path)
        with open(file_path, "r", encoding="utf-8") as f:
            json_data = f.read()
        model = cls.model_validate_json(json_data)
        model.file_path = file_path
        return model

    def save_as_json(self, file_path: Optional[str] = None):
        if file_path is None:
            file_path = self.file_path
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(self.model_dump_json(indent=4))


Main_WebUI_Cfg_p = partial(Main_WebUI_Cfg, init=True)
Inference_WebUI_Cfg_p = partial(Inference_WebUI_Cfg, init=True)
API_Cfg_p = partial(API_Cfg, init=True)


class Cfg(BaseModel):
    train_webui_cfg: Main_WebUI_Cfg = Field(default_factory=Main_WebUI_Cfg_p)
    inference_webui_cfg: Inference_WebUI_Cfg = Field(default_factory=Inference_WebUI_Cfg_p)
    api_batch_cfg: API_Cfg = Field(default_factory=API_Cfg_p)
    file_path: str = Field(default="tools/cfgs/cfg.json", exclude=True)

    @classmethod
    def from_json(cls, file_path: str) -> "Cfg":
        if not os.path.exists(file_path):
            cls().save_as_json(file_path=file_path)
        with open(file_path, "r", encoding="utf-8") as f:
            json_data = f.read()
        model = cls.model_validate_json(json_data)
        model.file_path = file_path
        return model

    def save_as_json(self, file_path: Optional[str] = None):
        if file_path is None:
            file_path = self.file_path
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(self.model_dump_json(indent=4))


if __name__ == "__main__":
    Cfg()
    Speakers_Cfg()
